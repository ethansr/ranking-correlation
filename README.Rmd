# Re-analysis of _Ranking Visualizations of Correlation Using Weber's Law_

_Matthew Kay ([mjskay@uw.edu](mailto:mjskay@uw.edu))_

## Libraries needed for these analyses

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)           #default code chunk options
panderOptions("table.split.table", Inf)     #don't split wide tables in output
panderOptions("table.style", "rmarkdown")   #table style that's supported by github
```

```{r libraries, message=FALSE, warning=FALSE}
library(car)            #bcPower
library(ggplot2)
library(scales)
library(grid)
library(gtable)
library(plyr)
library(dplyr)
library(survival)       #Surv
library(gamlss)
library(gamlss.cens)    #cens
```

```{r setup2, include=FALSE}
source("src/openGraphSaveGraph.R")

#set up ggplot theme
theme_set(theme_bw())
theme_update(
    panel.grid.major=element_blank(), 
    panel.grid.minor=element_blank(),
    axis.line=element_line(color="black"),
    panel.border=element_blank()
)

#model fit plotting function
plot.residuals = function(m, log_x=FALSE, log_base=2, data_color = "#bfbfbf", sd_color="#d62d0e", dnorm_color="#066fff") {

    data = data.frame(
        eval(m$call$data),
        `Fitted JND`=fitted(m), 
        `Normalized Quantile Residuals`=resid(m), 
        check.names=FALSE)
        
    scale_location = data %>%
        group_by(interval=cut(`Fitted JND`, 15)) %>%
        summarize(
            `Fitted JND` = mean(as.numeric(strsplit(as.character(interval), '(\\(|\\,|\\])')[[1]][2:3])),
            standard_deviation = sd(`Normalized Quantile Residuals`)
            )

    if (log_x) {
        #change log base for easier to understand better output
        data$`Fitted JND` = data$`Fitted JND` / log(log_base) 
        scale_location$`Fitted JND` = scale_location$`Fitted JND` / log(log_base)
    }

    fitted_plot = ggplot(data, 
            aes(x=`Fitted JND`, y=`Normalized Quantile Residuals`)
        ) +
        geom_point(color=data_color, size=2, alpha=0.4) +
        geom_rug(sides="r", color=data_color) +
        geom_hline(y=0, linetype="dashed", alpha=0.5) + 
        geom_linerange(data=scale_location, aes(ymin=standard_deviation, ymax=-standard_deviation, y=NULL), color=sd_color, size=1.0) +
        ylim(-5.5,5.5)
    if (log_x) {
        fitted_plot = fitted_plot +
            scale_x_continuous(
                limits=c(min(data$`Fitted JND`), max(data$`Fitted JND`)),
                labels=trans_format(function(x) log_base^x, math_format(.x))) +
            geom_vline(y=log2(.45)) +
            annotation_logticks(sides="b")
    } else {
        fitted_plot = fitted_plot +
            xlim(min(data$`Fitted JND`), max(data$`Fitted JND`))
    }
    fitted_plot = ggplotGrob(fitted_plot)
        

    residual_hist = ggplot(data, aes(x=`Normalized Quantile Residuals`)) +
        stat_density(fill=data_color) +
        geom_vline(x=0, linetype="dashed", alpha=0.5) + 
        xlim(-5.5,5.5) +
        ylim(0,0.6) +
        stat_function(fun=dnorm, linetype="dashed", color=dnorm_color) +
        coord_flip()
    residual_hist = ggplotGrob(residual_hist)
    
    grid.newpage()
    gtable(heights=unit(1,"null"), widths=unit(c(3,5,1),c("lines","null","null"))) %>%
        gtable_add_grob(fitted_plot[,1:3], 1, 1) %>%
        gtable_add_grob(fitted_plot[,4:5], 1, 2) %>%
        gtable_add_grob(residual_hist[,4:5], 1, 3) %>%
        grid.draw()

}
```

# Load and clean data
```{r clean}
#read data
df = read.csv("data/master.csv")

#clean up columns from original data
df = df %>%
    rename(r = rbase)

#calculate values used to derive filtering conditions used in original paper
df = df %>%
    group_by(visandsign) %>%
    summarize(p_chance = sum(jnd > .45)/length(jnd)) %>%    #proportion of jnd worse than chance
    join(df)

df = df %>%
    group_by(visandsign, r, approach) %>%
    summarize(
        jnd_mad = mad(jnd),         #within-group median absolute deviation 
        jnd_median = median(jnd)
    ) %>%
    join(df)

#filters used by original paper: when either is true, that data was excluded
df$p_chance_cutoff = df$p_chance > .2   #visandsigns with > 20% observations of jnd worse than chance
df$mad_cutoff = abs(df$jnd - df$jnd_median) > 3 * df$jnd_mad    #observations > 3 mads from the median
```

# Original linear model
This is a linear model with the original filter on MAD (> 3 MADs from the
median) and visandsigns with > 20% JNDs worse than chance excluded.

```{r linear_model}
m.linear = gamlss(jnd ~ r * visandsign * approach,
    sigma.formula = ~ visandsign,
    data=filter(df, !mad_cutoff & !p_chance_cutoff)
    )
plot(m.linear)
```

```{r linear_model_plot, include=FALSE, cache=FALSE}
openGraph(7,5)
plot.residuals(m.linear)
saveGraph("output/residuals-linear", "pdf")
```

# Log-linear model
If we estimate a Box-Cox power transform from the original model, the
confidence interval for $\lambda$ includes 0 (log transform) and excludes 1
(identity, i.e. the linear model):

```{r boxcox}
bc = powerTransform(jnd ~ r * visandsign * approach, data=filter(df, !mad_cutoff & !p_chance_cutoff))
summary(bc)
```

This suggests a log-linear model (i.e., using log-transformed JND) may perform
better than the linear model. Such a model stabilizes the variance in residuals.
It also addresses the problem of the original model making nonsensical
predictions (like JNDs less than 0).

```{r log_linear_model}
m.loglinear = gamlss(jnd ~ r * visandsign * approach,
    sigma.formula = ~ visandsign,
    data=filter(df, !mad_cutoff & !p_chance_cutoff),
    family=LOGNO
    )
plot(m.loglinear)
```

```{r log_linear_model_plot, include=FALSE, cache=FALSE}
openGraph(7,5)
plot.residuals(m.loglinear, log_x=TRUE)
saveGraph("output/residuals-loglinear", "pdf")
```

Note that the residuals here are more well-behaved in terms of normality and
scale-location invariance. We also have lower AIC in the log-linear model (`r AIC(m.loglinear)`)
compared to the linear model (`r AIC(m.linear)`).

# Log-linear model with censoring
This model addresses outliers through censoring. We remove both filters used in
the original paper, and instead uses censoring for values that are close to or
worse than chance, or close to or past the ceiling of JND (from above) and the
floor of JND (from below).

First, we derive the thresholds used for censoring.

```{r censoring_parameters}
#censorship based on being above / below ceiling/floor , and chance
df = mutate(df,
    censoring_threshold = ifelse(approach == "below", 
                    pmin(r - .05, .4), 
                    pmin(.95 - r, .4)),
    not_censored = jnd < censoring_threshold,
    censored_jnd = pmin(jnd, censoring_threshold)
)
```

Then, we build the censored model.

```{r censored_model}
m.censored = gamlss(Surv(censored_jnd, not_censored) ~ r * visandsign * approach,
    sigma.formula = ~ visandsign,
    data=df,
    family=cens(LOGNO)
    )
plot(m.censored)
```

```{r censored_model_plot, include=FALSE, cache=FALSE}
openGraph(7,5)
plot.residuals(m.censored, log_x=TRUE)
saveGraph("output/residuals-censored", "pdf")
```

# Bayesian log-linear model with censoring

# Precision of estimation for random correlations rendered using each chart type
 

