# Re-analysis of _Ranking Visualizations of Correlation Using Weber's Law_

_Matthew Kay ([mjskay@uw.edu](mailto:mjskay@uw.edu))_

## Libraries needed for these analyses

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)           #default code chunk options
pander::panderOptions("table.split.table", Inf)     #don't split wide tables in output
pander::panderOptions("table.style", "rmarkdown")   #table style that's supported by github
memory.limit(4000)
```

```{r libraries, message=FALSE, warning=FALSE, cache=FALSE}
library(MASS)           #load first to prevent select() from masking dplyr::select()
library(car)            #bcPower
library(ggplot2)
library(scales)
library(grid)
library(gtable)
library(plyr)
library(dplyr)
library(survival)       #Surv
library(gamlss)
library(gamlss.cens)    #cens
library(pander)         #pander
```

```{r setup2, include=FALSE}
source("src/openGraphSaveGraph.R")

#set up ggplot theme
theme_set(theme_bw() + theme(
    panel.grid.major=element_blank(), 
    panel.grid.minor=element_blank(),
    axis.line=element_line(color="black"),
    panel.border=element_blank(),
    text=element_text(size=16),
    axis.text=element_text(size=rel(15/16)),
    axis.ticks.length=unit(8, "points"),
    line=element_line(size=1)
))

#model fit plotting function
plot.residuals = function(m, log_x=FALSE, log_base=2, data_color = "#999999", sd_color="#d62d0e", dnorm_color="#066fff") {

    data = data.frame(
        eval(m$call$data),
        `Fitted JND`=fitted(m), 
        `Normalized Quantile Residuals`=resid(m), 
        check.names=FALSE)
        
    scale_location = data %>%
        group_by(interval=cut(`Fitted JND`, 15)) %>%
        summarize(
            `Fitted JND` = mean(as.numeric(strsplit(as.character(interval), '(\\(|\\,|\\])')[[1]][2:3])),
            standard_deviation = sd(`Normalized Quantile Residuals`)
            )

    if (log_x) {
        #change log base for easier to understand better output
        data$`Fitted JND` = data$`Fitted JND` / log(log_base) 
        scale_location$`Fitted JND` = scale_location$`Fitted JND` / log(log_base)
    }

    fitted_plot = ggplot(data, 
            aes(x=`Fitted JND`, y=`Normalized Quantile Residuals`)
        ) +
        geom_point(color=data_color, size=2, alpha=0.25) +
        geom_rug(sides="r", color=data_color) +
        geom_hline(y=0, linetype="dashed", alpha=0.5) + 
        geom_linerange(data=scale_location, aes(ymin=standard_deviation, ymax=-standard_deviation, y=NULL), color=sd_color, size=1.0) +
        ylim(-5.5,5.5)
    if (log_x) {
        fitted_plot = fitted_plot +
            scale_x_continuous(
                limits=c(min(data$`Fitted JND`), max(data$`Fitted JND`)),
                labels=trans_format(function(x) log_base^x, math_format(.x))) +
            geom_vline(y=log2(.45)) +
            annotation_logticks(sides="b")
    } else {
        fitted_plot = fitted_plot +
            xlim(min(data$`Fitted JND`), max(data$`Fitted JND`))
    }
    fitted_plot = ggplotGrob(fitted_plot)
        

    residual_hist = ggplot(data, aes(x=`Normalized Quantile Residuals`)) +
        stat_density(fill=data_color) +
        geom_vline(x=0, linetype="dashed", alpha=0.5) + 
        xlim(-5.5,5.5) +
        ylim(0,0.6) +
        stat_function(fun=dnorm, linetype="dashed", color=dnorm_color) +
        coord_flip()
    residual_hist = ggplotGrob(residual_hist)
    
    grid.newpage()
    gtable(heights=unit(1,"null"), widths=unit(c(3,5,1),c("lines","null","null"))) %>%
        gtable_add_grob(fitted_plot[,1:3], 1, 1) %>%
        gtable_add_grob(fitted_plot[,4:5], 1, 2) %>%
        gtable_add_grob(residual_hist[,4:5], 1, 3) %>%
        grid.draw()

}

#model fitting plotting function for a given viz, showing residuals relative to r
plot.r.residuals = function(df, m, plotted_visandsign, log_y=FALSE, log_base=2, data_color = "#999999", sd_color = "#d62d0e") {
    #calculate residual variance
    residual_df = df %>%
        filter(visandsign == plotted_visandsign & !mad_cutoff) %>%
        cbind(prediction=predict(m, newdata=.)) %>%
        mutate(
            jnd = if (log_y) log(jnd, base=log_base) else jnd,
            prediction = if (log_y) prediction / log(log_base) else prediction
            ) %>%
        group_by(r) %>%
        summarize(
            sd = sd(jnd - prediction),
            jnd = mean(prediction)
        )
    
    #calculate fit line
    coefs = as.list(coef(m))
    fit_slope = coefs$r + coefs[[paste0("r:visandsign", plotted_visandsign)]]
    fit_intercept = coefs$`(Intercept)` + coefs[[paste0("visandsign", plotted_visandsign)]]
    
    #chance threshold
    chance = if (log_y) log(.45, base=log_base) else .45
    
    if (log_y) {
        df$jnd = log(df$jnd, base=log_base)
        fit_slope = fit_slope / log(log_base)
        fit_intercept = fit_intercept / log(log_base)
    }
    
    p = ggplot(
            filter(df, visandsign == plotted_visandsign & !mad_cutoff),
            aes(x=r, 
                y=jnd 
                )) + 
        geom_point(size=3, alpha=.25, color=data_color) + 
#        geom_hline(yintercept=chance, lty="dashed") +
        geom_abline(slope=fit_slope, intercept=fit_intercept, size=1.0) +
        geom_linerange(data=residual_df, aes(ymin=jnd - sd, ymax=jnd + sd, y=NULL), color=sd_color, size=1.0)
    
    if (log_y) {
        p = p + 
#            scale_y_continuous(labels=trans_format(function(x) log_base^x, math_format(.x))) +
            annotation_logticks(sides="l")
    }
    
    p
}

```

# Load and clean data
```{r clean}
#read data
df = read.csv("data/master.csv")

#clean up columns from original data
df = df %>%
    rename(r = rbase)

#calculate values used to derive filtering conditions used in original paper
df = df %>%
    group_by(visandsign) %>%
    summarize(p_chance = sum(jnd > .45)/length(jnd)) %>%    #proportion of jnd worse than chance
    join(df)

df = df %>%
    group_by(visandsign, r, approach) %>%
    summarize(
        jnd_mad = mad(jnd),         #within-group median absolute deviation 
        jnd_median = median(jnd)
    ) %>%
    join(df)

#filters used by original paper: when either is true, that data was excluded
df$p_chance_cutoff = df$p_chance > .2   #visandsigns with > 20% observations of jnd worse than chance
df$mad_cutoff = abs(df$jnd - df$jnd_median) > 3 * df$jnd_mad    #observations > 3 mads from the median

#approach should be coded as sum-to-zero so that other coefficients can be 
#interpreted as relative to the mean of both approaches. We relevel first 
#so that "below" is assigned 1 and "above" -1 (this is just to eliminate
#a double-negative so that the sign of the coefficient of approach is positive,
#makes interpretation slightly simpler)
df$approach = relevel(df$approach, "below")
contrasts(df$approach) = contr.sum
```

# Weber-style model versus linear model fit to all data
```{r weber_example}
weber_data_df = filter(df, visandsign == "scatterplotpositive", !mad_cutoff)

weber_df = weber_data_df %>%
    group_by(approach, r) %>%
    summarize(jnd = mean(jnd)) %>%
    group_by(r) %>%
    mutate(
        mean.jnd.within.r = mean(jnd),
        r_A = r + ifelse(approach == "above", 0.5, -0.5) * mean.jnd.within.r
    ) 


#openGraph(6,5)    
ggplot(
        weber_data_df,
        aes(x=r,
            y=jnd
            )) + 
    geom_point(size=3, alpha=.25, color="#999999") +  
    geom_point(data=weber_df, mapping=aes(color=approach, x=r), size=5, alpha=.99) +
    geom_segment(data=adjusted_weber_df, mapping=aes(color=approach, x=r, xend=r_A, yend=jnd), size=1, arrow=arrow(type="closed", length=unit(7, "point"))) +
    scale_color_manual(values=c("#d62d0e", "#066fff"), guide=FALSE) +
    xlim(0.2, 0.9) + ylim(0, 0.3)
#saveGraph("output/weber-comparison-1", "pdf")

#openGraph(6,5)    
ggplot(
        adjusted_weber_df,
        aes(x=r_A,
            y=jnd
            )) + 
    geom_point(mapping=aes(color=approach), size=5, alpha=0.99) +
    stat_smooth(method=lm, se=FALSE, size=1, linetype="dashed", color="black") +
    scale_color_manual(values=c("#d62d0e", "#066fff"), guide=FALSE) +
    xlim(0.2, 0.9) + ylim(0, 0.3)
#saveGraph("output/weber-comparison-2", "pdf")
    
    
m = lm(log(jnd) ~ r, data=weber_data_df)
#openGraph(6,5)
ggplot(
        weber_data_df,
        aes(x=r,
            y=jnd
            )) + 
    geom_point(size=3, alpha=.25, color="#999999") +  
    stat_smooth(method=lm, se=FALSE, size=1, linetype="dashed", color="black") +
    stat_function(fun=function(x) exp(coef(m)[[1]] + x*coef(m)[[2]]), color="black", linetype="dashed") +
    xlim(0.2, 0.9) + ylim(0, 0.3)
#saveGraph("output/weber-comparison-3", "pdf")

```

# Linear model
Let's see an example of approach:

```{r approach_example}
#openGraph(6,5)
ggplot(
        filter(df, visandsign == "parallelCoordinatesnegative", !mad_cutoff),
        aes(x=r, 
            y=jnd,
            color=approach
            )) + 
    geom_point(size=3, alpha=.15) +  
    stat_smooth(method=lm, se=FALSE, size=1, linetype="dashed") +
    stat_smooth(method=lm, se=FALSE, mapping=aes(group=NA), size=1, color="black", linetype="dashed") +
    scale_color_manual(values=c("#d62d0e", "#066fff")) +
    ylim(0,0.3)
#saveGraph("output/approach-example", "pdf")

```

This is a linear model with the original filter on MAD (> 3 MADs from the
median) and visandsigns with > 20% JNDs worse than chance excluded.

```{r linear_model}
m.linear = gamlss(jnd ~ r * visandsign * approach,
    sigma.formula = ~ visandsign,
    data=filter(df, !mad_cutoff & !p_chance_cutoff)
    )
plot(m.linear)
```

```{r linear_model_plot, include=FALSE, cache=FALSE}
#openGraph(7,5)
plot.residuals(m.linear)
#saveGraph("output/residuals-linear", "pdf")

#openGraph(4,5)
plot.r.residuals(df, m.linear, "scatterplotnegative")
#saveGraph("output/residuals-linear-by_r-scatterplotnegative", "pdf")
```

# Log-linear model
If we estimate a Box-Cox power transform from the original model, the
confidence interval for $\lambda$ includes 0 (log transform) and excludes 1
(identity, i.e. the linear model):

```{r boxcox}
bc = powerTransform(jnd ~ r * visandsign * approach, data=filter(df, !mad_cutoff & !p_chance_cutoff))
summary(bc)
```

This suggests a log-linear model (i.e., using log-transformed JND) may perform
better than the linear model. Such a model stabilizes the variance in residuals.
It also addresses the problem of the original model making nonsensical
predictions (like JNDs less than 0).

```{r log_linear_model}
df$approach_value = contrasts(df$approach)[df$approach] #make numeric so we can easily make predictions without it
m.loglinear = gamlss(jnd ~ r * visandsign * approach_value,
    sigma.formula = ~ visandsign,
    data=filter(df, !mad_cutoff & !p_chance_cutoff),
    family=LOGNO
    )
plot(m.loglinear)
```

```{r log_linear_model_plot, include=FALSE, cache=FALSE}
#openGraph(7,5)
plot.residuals(m.loglinear, log_x=TRUE)
#saveGraph("output/residuals-loglinear", "pdf")

#openGraph(4,5)
plot.r.residuals(df, m.loglinear, "scatterplotnegative", log_y=TRUE)
#saveGraph("output/residuals-loglinear-by_r-scatterplotnegative", "pdf")
```

Note that the residuals here are more well-behaved in terms of normality and
scale-location invariance. We also have lower AIC in the log-linear model (`r AIC(m.loglinear)`)
compared to the linear model: (`r AIC(m.linear)`).

```{r linear_vs_loglinear_AIC, eval=FALSE}
AIC(m.linear, m.loglinear)
```

```{r linear_vs_loglinear_AIC_2, echo=FALSE, results='asis'}
pander(AIC(m.linear, m.loglinear))
```


## Note on linear model with non-constant variance

Note that we could fit a linear model with variance proportional to r. This addresses the
problem of scale-location variance but does not address the problem of skewed residuals
(as the log fit does), again suggesting the log fit is a better description of the data:  

```{r linear_model_v}
m.linear_v = gamlss(jnd ~ r * visandsign * approach,
    sigma.formula = ~ r * visandsign,
    data=filter(df, !mad_cutoff & !p_chance_cutoff)
    )
plot(m.linear_v)
```

```{r linear_vs_loglinear_AIC, eval=FALSE}
AIC(m.linear, m.linear_v, m.loglinear)
```

```{r linear_vs_loglinear_AIC_2, echo=FALSE, results='asis'}
pander(AIC(m.linear, m.loglinear))
```


# Log-linear model with censoring
First let's explain censoring. 

```{r censored distribution example}
actual_data = data.frame(x = rnorm(5000))
breaks = seq(min(actual_data$x), max(actual_data$x), length=30)
c = breaks[which.min(abs(breaks - 0.5))]  #make sure c ~= 1 and lines up with histogram breaks

actual_data$censored = actual_data$x > c
observed_data = actual_data %>% 
    mutate(x = ifelse(censored, c, x),
           dist = "observed")
actual_data$dist = "actual"

censored_data = rbind(actual_data, observed_data)
censored_data_means = censored_data %>%
    group_by(dist) %>%
    summarize(x=mean(x))

#openGraph(7,7)
ggplot(censored_data, aes(
        x = x,
        fill = censored,
        )) + 
    stat_bin(breaks=breaks) +
    geom_vline(xintercept=c, size=1, linetype="dashed") +
    geom_vline(data=censored_data_means, mapping=aes(xintercept=x), size=1, linetype="dashed") +
    facet_wrap(~dist, ncol=1)
#saveGraph("output/censoring-explanation", "pdf")
```    

Next, let's look at some data to decide what to censor

```{r looking_at_data_for_censoring}
openGraph(7.5,4)
ggplot(
        filter(df, approach=="above", visandsign %in% c("linepositive","stackedareapositive")),
        aes(x=r, 
            y=log2(jnd),
            color=not_censored
    )) + 
    geom_point(alpha=.25, size=2) + 
    geom_hline(yintercept=log2(.45), lty="dashed") + 
    stat_function(fun=function(x) log2(1 - x), lty="dashed", color="black") +
    annotation_logticks(sides="l") +
    scale_y_continuous(
        limits=c(-6, -0.2),
        labels=trans_format(function(x) 2^x, math_format(.x))) +
    facet_wrap(~visandsign)
saveGraph("output/censored-data-from-above", "pdf")

openGraph(7.5,4)
ggplot(
        filter(df, approach=="below", visandsign %in% c("parallelCoordinatespositive","parallelCoordinatesnegative")),
        aes(x=r, 
            y=log2(jnd),
            color=not_censored
    )) + 
    geom_point(alpha=.25, size=2) + 
    geom_hline(yintercept=log2(.45), lty="dashed") + 
    stat_function(fun=function(x) log2(x), lty="dashed", color="black") + 
    annotation_logticks(sides="l") +
    scale_y_continuous(
        limits=c(-6, -0.2),
        labels=trans_format(function(x) 2^x, math_format(.x))) +
    facet_wrap(~visandsign)
saveGraph("output/censored-data-from-below", "pdf")
```

This model addresses outliers through censoring. We remove both filters used in
the original paper, and instead uses censoring for values that are close to or
worse than chance, or close to or past the ceiling of JND (from above) and the
floor of JND (from below).

First, we derive the thresholds used for censoring.

```{r censoring_parameters}
#censorship based on being above / below ceiling/floor , and chance
df = mutate(df,
    censoring_threshold = ifelse(approach == "below", 
                    pmin(r - .05, .4), 
                    pmin(.95 - r, .4)),
    not_censored = jnd <= censoring_threshold,
    censored_jnd = pmin(jnd, censoring_threshold)
)
```

Then, we build the censored model.

```{r censored_model}
m.censored = gamlss(Surv(censored_jnd, not_censored) ~ r * visandsign * approach_value,
    sigma.formula = ~ visandsign,
    data=df,
    family=cens(LOGNO)
    )
plot(m.censored)
```

Now let's compare to the non-censored model

```{r censored_verses_not_comparison}
m.loglinear.all = gamlss(jnd ~ r * visandsign * approach_value,
    sigma.formula = ~ visandsign,
    data=df,
    family=LOGNO
    )

#get fits from models
models = list(uncensored=m.loglinear.all, censored=m.censored)
model_fits = ldply(c("uncensored", "censored"), function(m) {
    expand.grid(r=c(.2, .9), visandsign=levels(df$visandsign)) %>%
    mutate(approach_value = 0) %>%
    cbind(prediction = predict(models[[m]], newdata=.)) %>%
    mutate(model = m)
})


openGraph(7.5,4)
ggplot(
        filter(df, visandsign %in% c("linepositive","donutpositive")),
        aes(x=r, 
            y=log2(jnd),
    )) + 
    geom_point(alpha=.25, size=3, color="#999999") + 
    geom_hline(yintercept=log2(.45), lty="dashed") + 
#    stat_function(fun=function(x) log2(1 - x), lty="dashed", color="black") +
    geom_line(data=filter(model_fits, visandsign %in% c("linepositive","donutpositive")), mapping=aes(y=prediction/log(2), color=model)) + 
    annotation_logticks(sides="l") +
    scale_y_continuous(
        limits=c(-6, -0.2),
        labels=trans_format(function(x) 2^x, math_format(.x))) +
    facet_wrap(~visandsign)
saveGraph("output/censor-bias", "pdf")


#compare fit and error of censored and non-censored regression on something near chance
#have to use survreg here because gamlss won't give se for unobserved approach_value == 0,
#the resulting model is the same.
censored_example_vis = "linenegative"
m.censored_se = survreg(formula = Surv(censored_jnd, not_censored) ~ r * approach_value, 
    dist = "lognormal",  
    data = filter(df, visandsign == censored_example_vis)) 

m.uncensored_se = lm(formula = log(jnd) ~ r * approach_value,   
    data = filter(df, visandsign == censored_example_vis))
    
newdata = data.frame(r=3:8/10, approach_value=0)
censored_pred = predict(m.censored_se, newdata=newdata, se.fit=TRUE) %>%
    data.frame(newdata, model="censored") %>%
    mutate(fit = log(fit))
uncensored_pred = predict(m.uncensored_se, newdata=newdata, se.fit=TRUE) %>%
    data.frame(newdata, model="uncensored")

rbind.fill(censored_pred, uncensored_pred) %>%
    mutate(
        uc = fit + se.fit*qnorm(.975),
        lc = fit - se.fit*qnorm(.975)
    ) %>%
    ggplot(
        aes(x=r, 
            y=fit / log(2),
            color=model
    )) + 
    geom_hline(yintercept=log2(.45), lty="dashed") + 
    geom_ribbon(mapping=aes(ymin=lc/log(2), ymax=uc/log(2), fill=model), color=NA, alpha=0.25) + 
    geom_line() + 
    annotation_logticks(sides="l") +
    scale_y_continuous(
        limits=c(-6, -0.2),
        labels=trans_format(function(x) 2^x, math_format(.x)))


```

```{r censored_model_plot, include=FALSE, cache=FALSE}
#openGraph(7,5)
plot.residuals(m.censored, log_x=TRUE)
#saveGraph("output/residuals-censored", "pdf")
```    



# Bayesian log-linear model with censoring


# Random slopes

Turns out there isn't a whole lot of variation in slopes, so the fact that we can't estimate it doesn't seem like that much of a problem:

```{r participant_slopes}
ggplot(
        df,
        aes(x=r, 
            y=log2(jnd)
    )) + 
    geom_point(alpha=.25, color="#999999", size=2) + 
    stat_smooth(method=lm, alpha=.1, mapping=aes(group=participant), se=FALSE) +
    geom_hline(yintercept=log(.45), lty="dashed") + 
    geom_hline(yintercept=log(.40), lty="dashed") + 
    stat_function(fun=function(x) log2(1 - x), lty="dashed", color="black") + 
    stat_function(fun=function(x) log2(x), lty="dashed", color="black") +
    facet_wrap(~visandsign)
```



# Precision of estimation for random correlations rendered using each chart type
 

